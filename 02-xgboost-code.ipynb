{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost 분류 문제 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 `xgb`로 불러서 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.feature_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # isort:skip\n",
    "from sklearn.ensemble import RandomForestClassifier  # isort:skip\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {\"png\", \"retina\"}\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "mpl.rcParams[\"figure.constrained_layout.use\"] = True\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path() / \"data\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    "\n",
    "[타이타닉 데이터셋](https://github.com/alexisperrier/packt-aml/blob/master/ch4/titanic.csv)을 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save the file\n",
    "titanic_path = data_dir / \"titanic.csv\"\n",
    "\n",
    "if not titanic_path.exists():\n",
    "    response = requests.get(\n",
    "        \"https://raw.githubusercontent.com/alexisperrier/packt-aml/master/ch4/titanic.csv\"\n",
    "    )\n",
    "    with open(titanic_path, \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_temp = pd.read_csv(titanic_path)\n",
    "\n",
    "titanic_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_temp[\"pclass\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_temp[\"pclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.histplot(titanic_temp[\"pclass\"].astype(str))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_temp[\"sex\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_temp[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(titanic_temp[\"age\"])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_temp[\"fare\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_temp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피처 중 `fare`와 `embarked`는 결손치가 각각 1개, 2개가 있는데, 결손치가 있는 데이터는 아예 제외하겠습니다.\n",
    "`age`에는 결손치가 많이 있고, 따로 처리하지 않고 그대로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_names = [\"age\", \"sibsp\", \"parch\", \"fare\"]\n",
    "categorical_feature_names = [\"pclass\", \"sex\", \"embarked\"]\n",
    "feature_names = numeric_feature_names + categorical_feature_names\n",
    "target_name = \"survived\"\n",
    "\n",
    "titanic_raw = (\n",
    "    pd.read_csv(\n",
    "        titanic_path,\n",
    "        usecols=(feature_names + [target_name]),\n",
    "        dtype={\n",
    "            **{k: \"category\" for k in categorical_feature_names},\n",
    "            \"survived\": float,\n",
    "        },\n",
    "    )\n",
    "    .dropna(subset=[\"fare\", \"embarked\"])\n",
    "    .reset_index(drop=True)\n",
    "    .reindex(columns=(feature_names + [target_name]))\n",
    ")\n",
    "\n",
    "titanic_raw.info()\n",
    "titanic_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피처의 타입별로 데이터를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.select_dtypes(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw.select_dtypes(exclude=\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카테고리를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, series in titanic_raw.select_dtypes(\"category\").items():\n",
    "    print(c, series.cat.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 세트와 평가 세트로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train_raw,\n",
    "    x_test_raw,\n",
    "    y_train_raw,\n",
    "    y_test_raw,\n",
    ") = sklearn.model_selection.train_test_split(\n",
    "    titanic_raw[feature_names],\n",
    "    titanic_raw[target_name],\n",
    "    test_size=0.2,\n",
    "    random_state=78,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_raw.shape, x_test_raw.shape, y_train_raw.shape, y_test_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "트리 모델이라 수치 데이터의 정규화는 하지 않고, 범주형 데이터의 one-hot 인코딩만 진행합니다.\n",
    "\n",
    "Scikit-learn에서 제공하는 `ColumnTransformer`, [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) 등을 쓸 수도 있지만, 열 이름을 다 떼어 버리기 때문에, 여기서는 `pd.get_dummies`를 이용하겠습니다.\n",
    "\n",
    "원래는 fit-transform 과정을 거쳐야 하지만, 간단하게 하기 위해 이미 정보를 안다고 가정하고 수동으로 처리하겠습니다.\n",
    "\n",
    "- `pclass`: 0, 1, 2\n",
    "- `sex`: `female`, `male` 2가지이지만, 2열로 하지 않고 `male`은 떼어 버림\n",
    "- `embarked`: C, Q, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titanic_one_hot_encoder(x, y=None):\n",
    "    numeric_df = x[numeric_feature_names]\n",
    "    one_hot_encoded_df = (\n",
    "        pd.get_dummies(x[categorical_feature_names])\n",
    "        .drop(columns=\"sex_male\")\n",
    "        .rename(columns=str.lower)\n",
    "    )\n",
    "    new_x = pd.concat((numeric_df, one_hot_encoded_df), axis=1)\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = titanic_one_hot_encoder(x_train_raw)\n",
    "x_test = titanic_one_hot_encoder(x_test_raw)\n",
    "y_train = y_train_raw.copy()\n",
    "y_test = y_test_raw.copy()\n",
    "\n",
    "display(x_train.head())\n",
    "display(x_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ [Min-max scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), [정규화](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) 등을 할 때엔, 학습 세트와 평가 세트를 따로 하지 않도록 조심해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "example_train_set = 5 * rng.random((20, 2)) + 5\n",
    "example_test_set = 5 * rng.random((3, 2)) + 5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(example_train_set[:, 0], example_train_set[:, 1], \".\")\n",
    "ax.plot(example_test_set[:, 0], example_test_set[:, 1], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong!\n",
    "scaled_train_set = MinMaxScaler().fit_transform(example_train_set)\n",
    "scaled_test_set = MinMaxScaler().fit_transform(example_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(scaled_train_set[:, 0], scaled_train_set[:, 1], \".\")\n",
    "ax.plot(scaled_test_set[:, 0], scaled_test_set[:, 1], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_train_set = scaler.fit_transform(example_train_set)\n",
    "scaled_test_set = scaler.transform(example_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(scaled_train_set[:, 0], scaled_train_set[:, 1], \".\")\n",
    "ax.plot(scaled_test_set[:, 0], scaled_test_set[:, 1], \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 (기본 XGBoost API 이용)\n",
    "\n",
    "[XGBoost Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"eta\": 0.1,  # {0.3} learning rate\n",
    "    \"gamma\": 1.0,  # {0} Minimum loss reduction\n",
    "    \"max_depth\": 5,  # {6}\n",
    "    \"subsample\": 0.5,  # {1} Row (sample) subsample ratio\n",
    "    \"colsample_bytree\": 0.5,  # {1} Column (feature) subsample ratio\n",
    "    \"lambda\": 1,  # {1} L2 regularization term\n",
    "    \"verbosity\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(y_true, y_pred > 0.5),\n",
    "        \"log_loss\": sklearn.metrics.log_loss(y_true, y_pred),\n",
    "        \"roc_auc\": sklearn.metrics.roc_auc_score(y_true, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost는 자체적으로 `DMatrix` 형식의 데이터를 씁니다. ([Core Data Structure](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.core))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 200\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round=num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = bst.predict(dtrain)\n",
    "y_pred_test = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\", get_scores(y_train, y_pred_train))\n",
    "print(\"Test\", get_scores(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random guessing score\n",
    "print(\n",
    "    \"0.49, 0.51 random guess\",\n",
    "    get_scores(y_train, 0.49 + 0.02 * np.random.randint(2, size=y_train.shape)),\n",
    ")\n",
    "print(\n",
    "    \"0.01, 0.99 random guess\",\n",
    "    get_scores(y_train, 0.01 + 0.98 * np.random.randint(2, size=y_train.shape)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost에서 기본으로 제공하는 `plot_importance`를 이용해 F-score를 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "xgb.plot_importance(bst, importance_type=\"total_gain\", ax=ax)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부스터(학습된 모델)를 저장하고 불러오거나, 학습을 이어할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 200\n",
    "num_round_per_loop = 5\n",
    "\n",
    "bst = None\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for i in range(num_round_per_loop, num_round + 1, num_round_per_loop):\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=num_round_per_loop, xgb_model=bst)\n",
    "\n",
    "    y_pred_train = bst.predict(dtrain)\n",
    "    y_pred_test = bst.predict(dtest)\n",
    "    train_scores.append(\n",
    "        {\n",
    "            \"iteration\": i,\n",
    "            **{f\"train_{k}\": v for k, v in get_scores(y_train, y_pred_train).items()},\n",
    "        }\n",
    "    )\n",
    "    test_scores.append(\n",
    "        {\n",
    "            \"iteration\": i,\n",
    "            **{f\"test_{k}\": v for k, v in get_scores(y_test, y_pred_test).items()},\n",
    "        }\n",
    "    )\n",
    "\n",
    "train_scores_df = pd.DataFrame(train_scores).set_index(\"iteration\")\n",
    "test_scores_df = pd.DataFrame(test_scores).set_index(\"iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((train_scores_df, test_scores_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "train_scores_df.plot(marker=\".\", ax=ax)\n",
    "ax.set_prop_cycle(None)  # reset color cycle\n",
    "test_scores_df.plot(marker=\".\", ls=\":\", ax=ax)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "검증 세트를 주면 검증 세트의 점수가 더 나아지지 않으면 일찍 학습을 끊을수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train_train,\n",
    "    x_train_valid,\n",
    "    y_train_train,\n",
    "    y_train_valid,\n",
    ") = sklearn.model_selection.train_test_split(x_train, y_train)\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train_train, label=y_train_train)\n",
    "dvalid = xgb.DMatrix(x_train_valid, label=y_train_valid)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 200\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_round,\n",
    "    evals=[(dvalid, \"validset\")],\n",
    "    early_stopping_rounds=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn wrapper API\n",
    "\n",
    "XGBoost에서는 scikit-learn의 pipeline 등에 넣어서 scikit-learn의 estimator처럼 쓸 수 있는 wrapper 인터페이스를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_transformer = FunctionTransformer(titanic_one_hot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_transformer.fit(x_train_raw)\n",
    "\n",
    "one_hot_encoded_column_names = titanic_transformer.transform(x_train_raw).columns\n",
    "\n",
    "display(titanic_transformer.transform(x_train_raw).head())\n",
    "display(titanic_transformer.transform(x_test_raw).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"n_estimators\": 200,\n",
    "    \"learning_rate\": 0.3,  # {0.3} learning rate\n",
    "    \"gamma\": 1.0,  # {0} Minimum loss reduction\n",
    "    \"max_depth\": 5,  # {6}\n",
    "    \"subsample\": 0.5,  # {1} Row (sample) subsample ratio\n",
    "    \"colsample_bytree\": 1.0,  # {1} Column (feature) subsample ratio\n",
    "    \"reg_lambda\": 1,  # {1} L2 regularization term\n",
    "    \"verbosity\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"one_hot\", FunctionTransformer(titanic_one_hot_encoder)),\n",
    "        # (\"mean_imputer\", SimpleImputer()),\n",
    "        # (\"dim_reducer\", SelectKBest(sklearn.feature_selection.chi2)),\n",
    "        (\"clf\", XGBClassifier(**params)),\n",
    "        # (\"clf\", HistGradientBoostingClassifier()),\n",
    "    ]\n",
    ")\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(\n",
    "    x_train_raw, y_train_raw\n",
    ")\n",
    "\n",
    "x_test = x_test_raw.copy()\n",
    "y_test = y_test_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(x_train, y_train)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(y_test, pipe.predict_proba(x_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    # \"dim_reducer__k\": [5, 7],\n",
    "    \"clf__learning_rate\": [0.01, 0.03, 0.1],\n",
    "    \"clf__colsample_bytree\": [0.3, 0.5, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, param_grid, scoring=\"neg_log_loss\")\n",
    "\n",
    "gs.fit(x_train, y_train)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(y_test, gs.predict_proba(x_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_[\"clf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = gs.best_estimator_[\"clf\"].get_booster()\n",
    "importance = pd.Series(bst.get_score(importance_type=\"total_gain\"))\n",
    "# Restore feature names\n",
    "if \"dim_reducer\" in gs.best_estimator_.named_steps:\n",
    "    importance = importance.rename(\n",
    "        {\n",
    "            f\"f{i}\": one_hot_encoded_column_names[ci]\n",
    "            for i, ci in enumerate(\n",
    "                gs.best_estimator_[\"dim_reducer\"].get_support(indices=True)\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "importance.sort_values().plot.barh(ax=ax)\n",
    "ax.set(title=\"Feature importance (total gain)\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.6.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
